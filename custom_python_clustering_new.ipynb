{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "import pandas as pd\n",
    "import copy\n",
    "from itertools import cycle, islice\n",
    "from sklearn import cluster, datasets, mixture\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "plt.style.use('default')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin here to import PCA and UMAP and Leiden Cluster Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_analysis = pd.read_csv('pca_results.csv', header = 0)\n",
    "pca_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "pcx = '0'\n",
    "pcy = '1'\n",
    "ax.scatter(pca_analysis[pcx], pca_analysis[pcy])\n",
    "ax.set_xlabel(pcx)\n",
    "ax.set_ylabel(pcy)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pcs = 7\n",
    "values_matrix = pca_analysis.values[:, 0:n_pcs]\n",
    "print(values_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming values_matrix is your data matrix\n",
    "\n",
    "mean = np.mean(values_matrix, axis=0)\n",
    "std_dev = np.std(values_matrix, axis=0)\n",
    "\n",
    "# Calculate the bounds for 3 standard deviations\n",
    "lower_bound = mean - 3 * std_dev\n",
    "upper_bound = mean + 3 * std_dev\n",
    "\n",
    "# Create 23 bins within the bounds and add 2 bins for the outliers\n",
    "bins = np.empty((10, values_matrix.shape[1]))\n",
    "for col in range(values_matrix.shape[1]):\n",
    "    bins[1:-1, col] = np.linspace(lower_bound[col], upper_bound[col], num=8)\n",
    "    bins[0, col] = -np.inf  # Bin for values below lower bound\n",
    "    bins[-1, col] = np.inf  # Bin for values above upper bound\n",
    "\n",
    "# Digitize the values\n",
    "digitized = np.empty_like(values_matrix)\n",
    "for col in range(values_matrix.shape[1]):\n",
    "    digitized[:, col] = np.digitize(values_matrix[:, col], bins=bins[:, col])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# digitized now contains the indices of the bins to which each value belongs\n",
    "print(np.shape(bins))\n",
    "print(np.shape(digitized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count the number of points in each bin combo. We have 25 bins for each x,y\n",
    "dict25 = {}\n",
    "for row in digitized:\n",
    "    str_row = ' '.join(map(str, row.astype(int)))\n",
    "    if  str_row not in dict25.keys():\n",
    "        dict25[str_row] = 1\n",
    "    else:\n",
    "        dict25[str_row] = dict25[str_row] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dict25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to probabilities\n",
    "total_counts = sum(dict25.values())\n",
    "dict25_sp = {}\n",
    "for k, v in dict25.items():\n",
    "    dict25_sp[k] = v / total_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have to sort it from highest probability to lowest in the txt output\n",
    "\n",
    "# Sort the dictionary by value in descending order\n",
    "sorted_dict = dict(sorted(dict25_sp.items(), key=lambda x: x[1], reverse=True))\n",
    "#sorted_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering Algorithm Results added here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the clustering algorithm read in from the txt file vector data is the dimension space after clustering\n",
    "microstates = pd.read_csv( \"scanpy_pcs.txt.negmap\" , sep=\"|\" , skiprows= [1])\n",
    "microstates.columns = [col.strip() for col in microstates.columns]\n",
    "microstates[\"Vector\"] = microstates[\"Vector\"].apply(lambda v: np.array(v.strip().strip(\"[|]\").split(), dtype= int))\n",
    "microstates.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "microstates.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "done to ensure original files are not modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find unique Pk in microstates\n",
    "unique_pk_micro = microstates[\"Pk\"].unique()\n",
    "print(len(unique_pk_micro))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the highest and lowest probabilities\n",
    "highest_prob = microstates['Prob'].max()\n",
    "lowest_prob = microstates['Prob'].min()\n",
    "\n",
    "highest_prob, lowest_prob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scanpy_pcs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is the another file not sure what it is but it probably has the clusters birth and death info again "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we want to relabel the Pk value of the Pk centers because we had to kill them during persistent homology algorithm\n",
    "#this is the proabalistic map, the 0.000379075 means 1/2638\n",
    "peaks = pd.read_csv(\"scanpy_pcs.txt\" , sep=\"|\" , skiprows= [1])\n",
    "peaks.columns = [col.strip() for col in peaks.columns]\n",
    "\n",
    "\n",
    "#relabel the Pk values using Birth State Index\n",
    "clusters_ids = peaks[\"Birth State Index\"].unique()\n",
    "\n",
    "for peak in clusters_ids:\n",
    "    microstates['Pk'][peak] = peak\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  find unique Pk again after relabeling\n",
    "unique_pk = microstates[\"Pk\"].unique()\n",
    "print(len(unique_pk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group dataframe by \"Pk\" and sum the \"Prob\" column for each group\n",
    "cluster_probs = microstates.groupby('Pk')['Prob'].sum().to_dict()\n",
    "cluster_probs = dict(sorted(cluster_probs.items(), key=lambda x: x[1])) #, reverse = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find Pk value for a given vector element\n",
    "def find_pk_for_vector_element(dataframe, vector_element):\n",
    "    for index, row in dataframe.iterrows():\n",
    "        if np.array_equal(row['Vector'], vector_element):\n",
    "            return row['Pk']\n",
    "    return None\n",
    "\n",
    "pks_for_data = []\n",
    "for i in range(len(digitized)):\n",
    "    pks_for_data.append(find_pk_for_vector_element(microstates, digitized[i].astype(int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pks_for_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  find unique Pk again after relabeling\n",
    "unique_pk = microstates[\"Pk\"].unique()\n",
    "print(len(unique_pk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "frequency = Counter(pks_for_data)\n",
    "\n",
    "# Extract keys where values are less than 5\n",
    "keys_less_than = [key for key, value in frequency.items() if value < 5] #ari = 0.89 for <5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#based on this keeping the pk with highest probability\n",
    "#making a new df to store the aggregated data\n",
    "aggregated_data = microstates.groupby('Pk').apply(lambda x: x.loc[x['Prob'].idxmax()]).reset_index(drop=True)\n",
    "\n",
    "# Displaying the first few rows of the new aggregated DataFrame with max probability\n",
    "aggregated_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find unique Pk again after relabeling\n",
    "unique_pk_agg = aggregated_data[\"Pk\"].unique()\n",
    "print(len(unique_pk_agg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregated_data shape\n",
    "aggregated_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the aggregated data to a csv file\n",
    "aggregated_data.to_csv('aggregated_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "same shape as the unique vales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging 'microstates.csv' and 'peaks.csv' on 'Pk' and 'Birth State Index'\n",
    "merged_data = pd.merge(aggregated_data, peaks, left_on='Pk', right_on='Birth State Index', how='left')\n",
    "\n",
    "# Displaying the first few rows of the merged DataFrame\n",
    "print(merged_data.shape)\n",
    "merged_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find missing values in the merged data\n",
    "merged_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are Prob and Birth Probability the same for each row?\n",
    "merged_data['Prob'].equals(merged_data['Birth Probability'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique values Pk in merged data\n",
    "unique_pk_merged = merged_data[\"Pk\"].unique()\n",
    "print(len(unique_pk_merged))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "making new dataframe with pk values labelled for each row to the pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the Pk values as a new column\n",
    "pca_analysis['Pk'] = pks_for_data  # pks_for_data should be a list with the same length as pca_analysis\n",
    "\n",
    "# Make 'Pk' the first column by reordering the columns\n",
    "cols = ['Pk'] + [col for col in pca_analysis.columns if col != 'Pk']\n",
    "pca_analysis = pca_analysis[cols]\n",
    "\n",
    "# save the pca_analysis to a csv file\n",
    "pca_analysis.to_csv('pca_analysis.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows of the DataFrame to verify the column order\n",
    "pca_analysis.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the unique Pk in pca_analysis\n",
    "unique_pk_pca = pca_analysis[\"Pk\"].unique()\n",
    "print(len(unique_pk_pca))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I did a couple pf merges and this is the only one that Birth Probability and the Prob in both columns match "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the merged data to a csv file\n",
    "merged_data.to_csv('merged_data.csv', index=False)\n",
    "\n",
    "# load the merged data from the csv file\n",
    "final_data = pd.read_csv('merged_data.csv')\n",
    "final_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the 'Vector' column represents the dimensions made during clustering, we could consider each element of these vectors as features.\n",
    "we can then compute the Pearson correlation matrix. This matrix will show the correlation between each pair of features across all clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "# Check the number of unique Pk values to ensure there are 379\n",
    "num_unique_pks = pca_analysis['Pk'].nunique()\n",
    "print(num_unique_pks)\n",
    "\n",
    "# For each Pk, compute the mean of the vector dimensions\n",
    "pk_means = pca_analysis.groupby('Pk').mean()\n",
    "\n",
    "# We need a correlation matrix where each Pk value correlates with every other Pk value\n",
    "# To achieve this, we use the transpose of the pk_means dataframe to compute the correlation\n",
    "# This will treat each Pk as a separate entity and compute correlations between their mean vector values\n",
    "pk_correlation_matrix_final = pk_means.T.corr()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "# Convert the 'Vector' column into actual lists\n",
    "final_data['Vector'] = final_data['Vector'].apply(lambda x: [int(num) for num in x.strip('[]').split()])\n",
    "\n",
    "# Convert the 'Vector' lists into separate columns\n",
    "vector_df = pd.DataFrame(final_data['Vector'].tolist())\n",
    "\n",
    "# Incorporate the 'Prob' column if it's needed for the analysis\n",
    "# For now, we'll focus on the vector data\n",
    "# Combine the 'Pk' column with the vector dataframe\n",
    "combined_df = pd.concat([final_data[['Pk', 'Prob']], vector_df], axis=1)\n",
    "\n",
    "# To ensure Pk values are treated as unique identifiers (like strings), we'll convert them to string type\n",
    "#combined_df['Pk'] = combined_df['Pk'].astype(str)\n",
    "\n",
    "# Check the number of unique Pk values to ensure there are 309\n",
    "num_unique_pks = combined_df['Pk'].nunique()\n",
    "print(num_unique_pks)\n",
    "\n",
    "# For each Pk, compute the mean of the vector dimensions\n",
    "pk_means = combined_df.groupby('Pk').mean()\n",
    "\n",
    "# We need a correlation matrix where each Pk value correlates with every other Pk value\n",
    "# To achieve this, we use the transpose of the pk_means dataframe to compute the correlation\n",
    "# This will treat each Pk as a separate entity and compute correlations between their mean vector values\n",
    "pk_correlation_matrix_final = pk_means.T.corr()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pk_correlation_matrix_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set the size of the heatmap\n",
    "plt.figure(figsize=(20, 15))\n",
    "\n",
    "# Create a heatmap to visualize the correlation matrix\n",
    "sns.heatmap(pk_correlation_matrix_final, cmap='coolwarm')\n",
    "\n",
    "# Set the title of the heatmap\n",
    "plt.title('Heatmap of Pearson Correlation between Pk Values')\n",
    "\n",
    "# Show the heatmap\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try this for blocks that are less than 5/2638"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the threshold probability value\n",
    "threshold_prob = 5 / 2638\n",
    "\n",
    "# Filter out the rows where the probability is less than the threshold\n",
    "filtered_df = combined_df[combined_df['Prob'] < threshold_prob]\n",
    "\n",
    "# For the filtered Pk values, create a new dataframe for correlation calculation\n",
    "# We'll use the mean of the vector dimensions for each Pk, as we did before\n",
    "filtered_pk_means = filtered_df.groupby('Pk').mean()\n",
    "\n",
    "# Compute the Pearson correlation matrix for this filtered set of Pk values\n",
    "filtered_pk_correlation_matrix = filtered_pk_means.T.corr()\n",
    "\n",
    "# Display the shape of the new correlation matrix to confirm its size\n",
    "print(filtered_pk_correlation_matrix.shape)\n",
    "\n",
    "filtered_pk_correlation_matrix.head()  # Displaying part of the matrix for a glimpse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a heatmap for the filtered correlation matrix\n",
    "plt.figure(figsize=(20, 15))\n",
    "sns.heatmap(filtered_pk_correlation_matrix, cmap='coolwarm')\n",
    "plt.title('Heatmap of Pearson Correlation for Filtered Pk Values (Prob < 5/2638)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the correlation matrix to a csv file\n",
    "filtered_pk_correlation_matrix.to_csv('filtered_pk_correlation_matrix.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the threshold probability value\n",
    "threshold_prob = 5 / 2638\n",
    "\n",
    "# Filter out the rows where the probability is less than the threshold\n",
    "low_prob_pks = final_data[final_data['Prob'] < threshold_prob]\n",
    "\n",
    "# To ensure Pk values are treated as unique identifiers (like strings), we'll convert them to string type\n",
    "#low_prob_pks['Pk'] = low_prob_pks['Pk'].astype(str)\n",
    "\n",
    "# Display the first few rows of low probability Pks for a glimpse\n",
    "print(low_prob_pks.shape)\n",
    "low_prob_pks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# Convert the vector lists into a DataFrame for distance calculations\n",
    "vector_df = pd.DataFrame(low_prob_pks['Vector'].tolist(), index=low_prob_pks['Pk'])\n",
    "\n",
    "# Compute the Manhattan (cityblock) distances between all pairs of low-probability Pks\n",
    "manhattan_distances = cdist(vector_df, vector_df, metric='cityblock')\n",
    "\n",
    "# Convert the distance matrix to a DataFrame for easier manipulation\n",
    "distance_df = pd.DataFrame(manhattan_distances, index=vector_df.index, columns=vector_df.index)\n",
    "\n",
    "# Display the first few rows of the distance matrix for a glimpse\n",
    "print(distance_df.shape)\n",
    "distance_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the distance matrix to a csv file\n",
    "distance_df.to_csv('distance_matrix.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see if the distance_df and filtered_pk_correlation_matrix have the same column names\n",
    "print(distance_df.columns.equals(filtered_pk_correlation_matrix.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensuring that Pk values are consistent and exist in all dataframes\n",
    "\n",
    "# Check if all Pks in combined_df are in the distance and correlation matrices\n",
    "pks_in_combined_df = set(combined_df['Pk'])\n",
    "pks_in_distance_matrix = set(distance_df.index)\n",
    "pks_in_correlation_matrix = set(filtered_pk_correlation_matrix.index)\n",
    "\n",
    "# Find Pks that are not common in all dataframes\n",
    "non_common_pks = (pks_in_combined_df - pks_in_distance_matrix) | (pks_in_combined_df - pks_in_correlation_matrix)\n",
    "\n",
    "# Display any non-common Pks\n",
    "non_common_pks # the only excluded blocks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the combined_df to include only the Pk values that are present in both the distance and correlation matrices\n",
    "common_pks = pks_in_distance_matrix.intersection(pks_in_correlation_matrix)\n",
    "print(len(common_pks))\n",
    "filtered_combined_df = combined_df[combined_df['Pk'].isin(common_pks)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing the merging process\n",
    "\n",
    "# Function to find the closest Pk for merging\n",
    "def find_closest_pk(pk, distance_df, correlation_df):\n",
    "    # Get distances and correlations for the given Pk\n",
    "    distances = distance_df.loc[pk]\n",
    "    correlations = correlation_df.loc[pk]\n",
    "\n",
    "    # Ignore the distance to itself by setting it to infinity\n",
    "    distances[pk] = float('inf')\n",
    "\n",
    "    # Find the minimum distance\n",
    "    min_distance = distances.min()\n",
    "    closest_pks = distances[distances == min_distance].index\n",
    "\n",
    "    # If more than one Pk is at the same minimum distance, choose the one with the highest correlation\n",
    "    if len(closest_pks) > 1:\n",
    "        closest_pk = correlations[closest_pks].idxmax()\n",
    "    else:\n",
    "        closest_pk = closest_pks[0]\n",
    "\n",
    "    return closest_pk\n",
    "\n",
    "# Merging function  for each cluster\n",
    "\n",
    "def merge_clusters(data, distance_df, correlation_df, threshold_prob):\n",
    "    merges = []\n",
    "    pks_to_merge = set(data[data['Prob'] < threshold_prob]['Pk'])\n",
    "\n",
    "    while pks_to_merge:\n",
    "        # Select a random Pk to start merging\n",
    "        current_pk = pks_to_merge.pop()\n",
    "        merge_count = 0\n",
    "        last_pk = None\n",
    "\n",
    "        # Continue merging until the probability threshold is met or no more Pks to merge\n",
    "        while data.loc[data['Pk'] == current_pk, 'Prob'].iloc[0] < threshold_prob and pks_to_merge:\n",
    "            closest_pk = find_closest_pk(current_pk, distance_df, correlation_df)\n",
    "\n",
    "            # Merge current Pk with the closest Pk\n",
    "            current_prob = data.loc[data['Pk'] == current_pk, 'Prob'].iloc[0]\n",
    "            closest_pk_prob = data.loc[data['Pk'] == closest_pk, 'Prob'].iloc[0]\n",
    "            new_prob = current_prob + closest_pk_prob\n",
    "\n",
    "            # Update merge count and last merged Pk\n",
    "            merge_count += 1\n",
    "            last_pk = closest_pk\n",
    "\n",
    "            # Update the dataframe with new probability\n",
    "            data.loc[data['Pk'] == current_pk, 'Prob'] = new_prob\n",
    "            data.loc[data['Pk'] == closest_pk, 'Prob'] = new_prob\n",
    "\n",
    "            # Remove the merged Pk from the set\n",
    "            pks_to_merge.discard(closest_pk)\n",
    "\n",
    "            # Update the current Pk if the closest Pk has a higher initial probability\n",
    "            if closest_pk_prob > current_prob:\n",
    "                current_pk = closest_pk\n",
    "\n",
    "        # Record the merge details\n",
    "        merges.append({\n",
    "            'Birth Prob': current_prob,\n",
    "            'State Index': current_pk,\n",
    "            'Final Index': last_pk,\n",
    "            'Pk': current_pk,\n",
    "            'Prob': new_prob,\n",
    "            'Merge Count': merge_count,\n",
    "            'Last Pk': last_pk\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(merges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold for the merge\n",
    "threshold = 5/2638\n",
    "# Perform the merging process\n",
    "merged_data_full = merge_clusters(combined_df, distance_df, filtered_pk_correlation_matrix, threshold)\n",
    "\n",
    "# Display the first few rows of the merge records\n",
    "print(merged_data_full.shape)\n",
    "merged_data_full.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the maximum and lowest probability in merged data\n",
    "max_prob = merged_data_full['Prob'].max()\n",
    "min_prob = merged_data_full['Prob'].min()\n",
    "\n",
    "max_prob, min_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# value counts of the merge count\n",
    "merged_data_full['Merge Count'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the unique Pk in the merged data\n",
    "unique_pk_merged = merged_data_full[\"Pk\"].unique()\n",
    "print(len(unique_pk_merged))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "b7f8fd0c5002388c6c3880e9c4024274951a5d941e05df8c2c771372d64f320d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
